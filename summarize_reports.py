#!/usr/bin/env python3
"""
Pathology Report and Consultation Notes Summarizer using Llama 3.2 and LangChain

This script reads pathology report and consultation note text files, generates concise summaries
with key information using the Llama 3.2 model via LangChain, performs quality control, generates
embeddings for the summaries, and converts these summaries into structured tabular data.

Author: Yujing Zou
Date: Dec, 2024
"""

import os
import json
import re
import argparse
import logging
import pickle
from typing import List, Dict, Any, Optional
import random

import pandas as pd
from tqdm import tqdm

# LangChain and Model-specific Imports
from langchain.vectorstores import Chroma
from langchain.embeddings import OpenAIEmbeddings
from langchain.prompts import ChatPromptTemplate
from langchain.output_parsers import StrOutputParser
from langchain.text_splitters import RecursiveCharacterTextSplitter
from langchain.chains import LLMChain
from langchain.runnables import RunnablePassthrough

# Model-specific imports
from langchain_ollama import OllamaEmbeddings, ChatOllama
from langchain_openai import ChatOpenAI
from langchain_google_genai import GoogleGenerativeAIEmbeddings, ChatGoogleGenerativeAI

# Configure Logging
logging.basicConfig(level=logging.INFO, format='%(levelname)s: %(message)s')
logger = logging.getLogger(__name__)

# Paths Configuration
PATHS = {
    "chroma_embeddings": "/path/to/chroma_embeddings",  # Update with your path
    "input_dir": "/path/to/input_reports",              # Directory containing .txt files
    "output_dir": "/path/to/output_dir",                # Directory to save outputs
    "log_file": "/path/to/output_dir/summarizer.log"    # Log file path
}

# Ensure output directory exists
os.makedirs(PATHS["output_dir"], exist_ok=True)

# Update Logging to File and Console
file_handler = logging.FileHandler(PATHS["log_file"])
file_handler.setLevel(logging.DEBUG)
formatter = logging.Formatter('%(asctime)s - %(levelname)s: %(message)s')
file_handler.setFormatter(formatter)
logger.addHandler(file_handler)

# Regular Expressions for Post-Processing
PATTERNS = {
    "Diagnosis": r"Diagnosis:\s*(.*)",
    "Tumor Size": r"Tumor Size:\s*([\d\.]+\s*\w+)",
    "Grade": r"Grade:\s*(\d+)",
    "Biomarkers": r"Biomarkers:\s*(.*)",
    "Patient Concerns": r"Patient Concerns:\s*(.*)",
    "Recommendations": r"Recommendations:\s*(.*)",
    "Follow-up Actions": r"Follow-up Actions:\s*(.*)"
}


def extract_tabular_data(summary: str, report_type: str) -> Dict[str, Any]:
    """
    Extracts key information from the summary using regex based on report type.

    Args:
        summary (str): The text summary generated by the model.
        report_type (str): Type of the report ('pathology_report' or 'consultation_notes').

    Returns:
        Dict[str, Any]: Extracted key information.
    """
    extracted = {}
    if report_type == "pathology_report":
        fields = ["Diagnosis", "Tumor Size", "Grade", "Biomarkers"]
    elif report_type == "consultation_notes":
        fields = ["Patient Concerns", "Recommendations", "Follow-up Actions"]
    else:
        logger.warning(f"Unknown report type: {report_type}")
        return extracted

    for key in fields:
        pattern = PATTERNS.get(key)
        if pattern:
            match = re.search(pattern, summary, re.IGNORECASE)
            if match:
                extracted[key] = match.group(1).strip()
            else:
                extracted[key] = None
    return extracted

def validate_extracted_data(data: Dict[str, Any], report_type: str) -> bool:
    """
    Validates the extracted tabular data.

    Args:
        data (Dict[str, Any]): Extracted data for a single report.
        report_type (str): Type of the report ('pathology_report' or 'consultation_notes').

    Returns:
        bool: True if data is valid, False otherwise.
    """
    if report_type == "pathology_report":
        required_fields = ["Diagnosis", "Tumor Size", "Grade", "Biomarkers"]
        # Validation rules
        size_pattern = r"^\d+(\.\d+)?\s*(mm|cm|in)$"
        grade_pattern = r"^[1-3]$"  # Assuming grades 1-3
    elif report_type == "consultation_notes":
        required_fields = ["Patient Concerns", "Recommendations", "Follow-up Actions"]
        # Validation rules can be added as needed
        size_pattern = None
        grade_pattern = None
    else:
        logger.warning(f"Unknown report type: {report_type}")
        return False

    # Check for presence of all required fields
    for field in required_fields:
        if not data.get(field):
            logger.debug(f"Missing field '{field}' in data: {data}")
            return False

    # Specific validations
    if report_type == "pathology_report":
        # Validate Tumor Size format
        if not re.match(size_pattern, data["Tumor Size"], re.IGNORECASE):
            logger.debug(f"Invalid Tumor Size format: {data['Tumor Size']}")
            return False

        # Validate Grade
        if not re.match(grade_pattern, data["Grade"], re.IGNORECASE):
            logger.debug(f"Invalid Grade value: {data['Grade']}")
            return False

    return True

def normalize_data(df: pd.DataFrame, report_type: str) -> pd.DataFrame:
    """
    Normalizes and cleans the extracted tabular data.

    Args:
        df (pd.DataFrame): Raw extracted data.
        report_type (str): Type of the report ('pathology_report' or 'consultation_notes').

    Returns:
        pd.DataFrame: Cleaned and normalized data.
    """
    if report_type == "pathology_report":
        def convert_size(size_str):
            match = re.match(r"([\d\.]+)\s*(mm|cm|in)", size_str, re.IGNORECASE)
            if not match:
                return None
            size, unit = match.groups()
            size = float(size)
            unit = unit.lower()
            if unit == "cm":
                return size * 10  # Convert to mm
            elif unit == "in":
                return size * 25.4  # Convert to mm
            return size  # mm

        df['Tumor Size (mm)'] = df['Tumor Size'].apply(convert_size)

        # Convert Grade to integer
        df['Grade'] = pd.to_numeric(df['Grade'], errors='coerce').astype('Int64')

        # Handle missing Biomarkers
        df['Biomarkers'] = df['Biomarkers'].fillna('None')

        # Drop original Tumor Size column
        df = df.drop(columns=['Tumor Size'])

    elif report_type == "consultation_notes":
        # Add normalization steps if needed
        pass

    return df

class ReportSummarizer:
    """
    Summarizes pathology reports and consultation notes, performs quality control,
    generates embeddings, and converts summaries into structured tabular data.
    """
    def __init__(self, prompts_dir: str, model_type: str = "local", temperature: float = 0.8):
        """
        Initializes the summarizer with the specified model and prompts.

        Args:
            prompts_dir (str): Path to the directory containing prompt JSON files.
            model_type (str): Type of model to use ('local', 'gpt', 'gemini').
            temperature (float): Sampling temperature for the model.
        """
        self.model_type = model_type.lower()
        self.temperature = temperature

        # Load prompts from separate files based on report type
        # Assuming prompts_dir contains prompt_<report_type>.json files
        if not os.path.isdir(prompts_dir):
            logger.error(f"Prompts directory path {prompts_dir} is not a directory.")
            raise ValueError(f"Prompts directory path {prompts_dir} is not a directory.")

        self.prompts = {}
        for report_type in ["pathology_report", "consultation_notes"]:
            prompt_file = os.path.join(prompts_dir, f"prompt_{report_type}.json")
            if os.path.isfile(prompt_file):
                with open(prompt_file, 'r') as f:
                    data = json.load(f)
                    self.prompts[report_type] = data.get("prompts", [])
                logger.info(f"Loaded {len(self.prompts[report_type])} prompts for {report_type}.")
            else:
                logger.warning(f"Prompt file {prompt_file} not found for report type {report_type}.")
                self.prompts[report_type] = []

        # Initialize the appropriate model
        if self.model_type == "local":
            self.model = ChatOllama(model="llama3.2:70b", temperature=self.temperature)
            embeddings = OllamaEmbeddings(model="nomic-embed-text")
        elif self.model_type == "gpt":
            self.model = ChatOpenAI(model="gpt-4", temperature=self.temperature)
            embeddings = OpenAIEmbeddings()
        elif self.model_type == "gemini":
            self.model = ChatGoogleGenerativeAI(model="gemini-1.5-flash", temperature=self.temperature)
            embeddings = GoogleGenerativeAIEmbeddings(model="models/embedding-001", task_type="retrieval_document")
        else:
            logger.error(f"Unsupported model type: {model_type}")
            raise ValueError(f"Unsupported model type: {model_type}")

        # Initialize Vector Store (optional since retrieval is not used)
        # Here, it's not essential, but initializing in case it's needed for embeddings
        self.vectorstore = Chroma(embedding_function=embeddings, persist_directory=PATHS["chroma_embeddings"])
        logger.info(f"Initialized vector store at {PATHS['chroma_embeddings']} using {model_type} embeddings.")

        # Initialize Prompt Templates
        self.chain_map = {}
        for report_type, prompt_list in self.prompts.items():
            if not prompt_list:
                continue
            # Select a random prompt from the list
            selected_prompt = random.choice(prompt_list)
            prompt = ChatPromptTemplate.from_template(selected_prompt)
            chain = (
                RunnablePassthrough.assign(context=lambda input: input["context"])
                | prompt
                | self.model
                | StrOutputParser()
            )
            self.chain_map[report_type] = chain

        logger.info(f"Initialized LLM chains for report types: {list(self.chain_map.keys())}")

    def summarize_report(self, report_text: str, report_type: str) -> Optional[str]:
        """
        Generates a summary for a single report.

        Args:
            report_text (str): The raw text of the report.
            report_type (str): Type of the report ('pathology_report' or 'consultation_notes').

        Returns:
            Optional[str]: The generated summary or None if generation failed.
        """
        try:
            if report_type not in self.chain_map:
                logger.warning(f"No prompts available for report type: {report_type}")
                return None

            chain = self.chain_map[report_type]
            summary = chain.invoke({"context": report_text})
            return summary.strip()
        except Exception as e:
            logger.error(f"Error generating summary for report type {report_type}: {e}")
            return None

    def process_reports(self, input_dir: str, output_dir: str) -> None:
        """
        Processes all .txt reports in the input directory.

        Args:
            input_dir (str): Directory containing .txt files.
            output_dir (str): Directory to save summaries, embeddings, and tabular data.
        """
        summaries = []
        tabular_data = []
        invalid_entries = []

        txt_files = [f for f in os.listdir(input_dir) if f.endswith(".txt")]
        logger.info(f"Found {len(txt_files)} .txt files to process.")

        for filename in tqdm(txt_files, desc="Processing Reports"):
            file_path = os.path.join(input_dir, filename)
            try:
                with open(file_path, 'r') as f:
                    report_text = f.read()

                # Determine report type from filename
                if filename.lower().startswith("pathology_report"):
                    report_type = "pathology_report"
                elif filename.lower().startswith("consultation_notes"):
                    report_type = "consultation_notes"
                else:
                    logger.warning(f"Unknown report type for file: {filename}. Skipping.")
                    invalid_entries.append({"file": filename, "reason": "Unknown report type"})
                    continue

                # Generate summary
                summary = self.summarize_report(report_text, report_type)
                if not summary:
                    logger.warning(f"Failed to generate summary for {filename}.")
                    invalid_entries.append({"file": filename, "reason": "Summary generation failed"})
                    continue

                # Extract tabular data
                extracted = extract_tabular_data(summary, report_type)

                # Validate extracted data
                if validate_extracted_data(extracted, report_type):
                    summaries.append({"file": filename, "summary": summary})
                    tabular_data.append(extracted)
                else:
                    logger.warning(f"Validation failed for {filename}. Summary may be incomplete.")
                    invalid_entries.append({"file": filename, "reason": "Validation failed"})
                    summaries.append({"file": filename, "summary": summary})  # Still save the summary

            except Exception as e:
                logger.error(f"Error processing {filename}: {e}")
                invalid_entries.append({"file": filename, "reason": str(e)})

        # Save Summaries
        summaries_df = pd.DataFrame(summaries)
        summaries_csv = os.path.join(output_dir, "summaries.csv")
        summaries_df.to_csv(summaries_csv, index=False)
        logger.info(f"Summaries saved to {summaries_csv}")

        # Save Tabular Data
        tabular_df = pd.DataFrame(tabular_data)
        tabular_csv = os.path.join(output_dir, "tabular_data.csv")
        tabular_df.to_csv(tabular_csv, index=False)
        logger.info(f"Tabular data saved to {tabular_csv}")

        # Normalize Tabular Data
        # Handle each report type separately
        for report_type in self.prompts.keys():
            if report_type == "pathology_report":
                if not {"Diagnosis", "Tumor Size", "Grade", "Biomarkers"}.issubset(tabular_df.columns):
                    logger.warning(f"Missing expected columns for {report_type}. Skipping normalization.")
                    continue
                df_subset = tabular_df[["Diagnosis", "Tumor Size", "Grade", "Biomarkers"]].copy()
                df_normalized = normalize_data(df_subset, report_type)
                normalized_csv = os.path.join(output_dir, f"tabular_data_normalized_{report_type}.csv")
                df_normalized.to_csv(normalized_csv, index=False)
                logger.info(f"Normalized tabular data saved to {normalized_csv}")
            elif report_type == "consultation_notes":
                if not {"Patient Concerns", "Recommendations", "Follow-up Actions"}.issubset(tabular_df.columns):
                    logger.warning(f"Missing expected columns for {report_type}. Skipping normalization.")
                    continue
                df_subset = tabular_df[["Patient Concerns", "Recommendations", "Follow-up Actions"]].copy()
                # Add normalization steps if needed
                df_normalized = df_subset  # Placeholder for potential normalization
                normalized_csv = os.path.join(output_dir, f"tabular_data_normalized_{report_type}.csv")
                df_normalized.to_csv(normalized_csv, index=False)
                logger.info(f"Normalized tabular data saved to {normalized_csv}")
            else:
                continue  # Handle other report types if added in the future

        # Save Invalid Entries Log
        if invalid_entries:
            invalid_df = pd.DataFrame(invalid_entries)
            invalid_log = os.path.join(output_dir, "invalid_entries.log")
            invalid_df.to_csv(invalid_log, index=False)
            logger.info(f"Invalid entries logged to {invalid_log}")

    def generate_embeddings(self, output_dir: str, embedding_model_type: Optional[str] = "openai") -> None:
        """
        Generates embeddings for the summaries and saves them.

        Args:
            output_dir (str): Directory where summaries.csv is located.
            embedding_model_type (Optional[str]): Type of embedding model to use ('openai', 'ollama', 'google').
        """
        summaries_csv = os.path.join(output_dir, "summaries.csv")
        embeddings_pickle = os.path.join(output_dir, "summary_embeddings.pkl")

        if not os.path.isfile(summaries_csv):
            logger.error(f"Summaries file not found at {summaries_csv}. Cannot generate embeddings.")
            return

        summaries_df = pd.read_csv(summaries_csv)
        logger.info(f"Loaded {len(summaries_df)} summaries for embedding generation.")

        # Initialize Embedding Model
        if embedding_model_type.lower() == "openai":
            embedding_model = OpenAIEmbeddings()
        elif embedding_model_type.lower() == "ollama":
            embedding_model = OllamaEmbeddings(model="nomic-embed-text")
        elif embedding_model_type.lower() == "google":
            embedding_model = GoogleGenerativeAIEmbeddings(model="models/embedding-001", task_type="retrieval_document")
        else:
            logger.error(f"Unsupported embedding model type: {embedding_model_type}. Using OpenAIEmbeddings by default.")
            embedding_model = OpenAIEmbeddings()

        # Generate Embeddings
        try:
            embeddings = embedding_model.embed_documents(summaries_df['summary'].tolist())
        except Exception as e:
            logger.error(f"Error generating embeddings: {e}")
            return

        # Add embeddings to DataFrame
        summaries_df['embedding'] = embeddings

        # Save Embeddings
        with open(embeddings_pickle, 'wb') as f:
            pickle.dump(summaries_df[['file', 'embedding']].to_dict(orient='records'), f)
        logger.info(f"Embeddings saved to {embeddings_pickle}")

def main():
    """
    Main function to execute the summarization pipeline.
    """
    parser = argparse.ArgumentParser(description="Pathology Report and Consultation Notes Summarizer using Llama 3.2 and LangChain")

    parser.add_argument(
        '--prompts_dir',
        type=str,
        required=True,
        help='Directory containing separate JSON prompt files for each report type.'
    )
    parser.add_argument(
        '--model_type',
        type=str,
        default='local',
        choices=['local', 'gpt', 'gemini'],
        help='Type of model to use for summarization: local (Llama 3.2 via Ollama), gpt (OpenAI GPT), gemini (Google Generative AI)'
    )
    parser.add_argument(
        '--temperature',
        type=float,
        default=0.8,
        help='Sampling temperature for the language model.'
    )
    parser.add_argument(
        '--input_dir',
        type=str,
        required=True,
        help='Directory containing .txt pathology report and consultation note files.'
    )
    parser.add_argument(
        '--output_dir',
        type=str,
        required=True,
        help='Directory to save summaries, embeddings, and tabular data.'
    )
    parser.add_argument(
        '--embedding_model',
        type=str,
        default='openai',
        choices=['openai', 'ollama', 'google'],
        help='Type of embedding model to use for generating embeddings: openai, ollama, google'
    )

    args = parser.parse_args()

    # Ensure output directory exists
    os.makedirs(args.output_dir, exist_ok=True)

    # Initialize Summarizer
    summarizer = ReportSummarizer(prompts_dir=args.prompts_dir, model_type=args.model_type, temperature=args.temperature)

    # Process Reports
    summarizer.process_reports(input_dir=args.input_dir, output_dir=args.output_dir)

    # Generate Embeddings
    summarizer.generate_embeddings(output_dir=args.output_dir, embedding_model_type=args.embedding_model)

if __name__ == "__main__":
    main()
